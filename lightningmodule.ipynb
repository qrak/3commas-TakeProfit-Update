{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONsvAXROa2MwyBMHyssYKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qrak/3commas-TakeProfit-Update/blob/master/lightningmodule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "YIlMEsF6hNRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "zfCNcecTgF3k",
        "outputId": "8949cd34-787f-4b2e-ac83-f33239eac31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with input_size=3, hidden_size=16, num_layers=2, dropout=0.1\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:474: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MisconfigurationException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5f1f6da8ef26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerator_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_gpu_accelerator_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_parallel_devices_and_init_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# 3. Instantiate ClusterEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mAcceleratorRegistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accelerator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             ]\n\u001b[0;32m--> 561\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0;34mf\"`{accelerator_cls.__qualname__}` can not run on your system\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                 \u001b[0;34m\" since the accelerator is not available. The following accelerator(s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMisconfigurationException\u001b[0m: `CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "\n",
        "class LSTMNet(pl.LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(64, output_size)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.dropout1(out)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.dropout2(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs.unsqueeze(1))\n",
        "        loss = self.loss(outputs, labels.view(-1, 1))\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs.unsqueeze(1))\n",
        "        loss = self.loss(outputs, labels.view(-1, 1))\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs.unsqueeze(1))\n",
        "        loss = self.loss(outputs, labels.view(-1, 1))\n",
        "        self.log('test_loss', loss)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load data into dataframe\n",
        "    df = pd.read_csv('BTC_USDT_5m_2015-02-01_now_binance.csv')\n",
        "    df = df.dropna()\n",
        "    scaler = StandardScaler()\n",
        "    # Extract the target variable (close price)\n",
        "    y = df.iloc[:, 4].values\n",
        "\n",
        "    # Extract the input features\n",
        "    X = df.iloc[:, 1:4].values\n",
        "    X = scaler.fit_transform(X)\n",
        "    # Perform feature selection\n",
        "    k = 3  # Number of features to select\n",
        "    selector = SelectKBest(f_regression, k=k)\n",
        "    X = selector.fit_transform(X, y)\n",
        "    # Split the data into training, validation, and test sets\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, shuffle=False)\n",
        "    # Move the data to the specified device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
        "    # Define the number of folds for cross-validation\n",
        "    num_folds = 5\n",
        "\n",
        "    # Define the hyperparameters to search over\n",
        "    input_sizes = [X.shape[1]]\n",
        "    hidden_sizes = [16, 32, 64]\n",
        "    num_layers_list = [2, 4, 6]\n",
        "    dropout_sizes = [0.1]\n",
        "\n",
        "    num_epochs = 100\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "    train_loader = None\n",
        "    optimizer = None\n",
        "    criterion = None\n",
        "    val_loader = None\n",
        "    best_hyperparams = None\n",
        "    for input_size in input_sizes:\n",
        "        for hidden_size in hidden_sizes:\n",
        "            for num_layers in num_layers_list:\n",
        "                for dropout_size in dropout_sizes:\n",
        "                    print(f\"Training model with input_size={input_size}, hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout_size}\")\n",
        "                    val_losses = []\n",
        "                    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "                    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_val)):\n",
        "                        print(f\"Fold {fold + 1}/{num_folds}\")\n",
        "                        # Split the data into training and validation sets for this fold\n",
        "                        X_fold_train, y_fold_train = X_train_val[train_idx], y_train_val[train_idx]\n",
        "                        X_fold_val, y_fold_val = X_train_val[val_idx], y_train_val[val_idx]\n",
        "\n",
        "                        # Create the PyTorch datasets and data loaders\n",
        "                        train_dataset = TensorDataset(torch.tensor(X_fold_train, dtype=torch.float32).to(device),\n",
        "                                                      torch.tensor(y_fold_train, dtype=torch.float32).to(device))\n",
        "                        val_dataset = TensorDataset(torch.tensor(X_fold_val, dtype=torch.float32).to(device),\n",
        "                                                    torch.tensor(y_fold_val, dtype=torch.float32).to(device))\n",
        "                        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=False)\n",
        "                        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=False)\n",
        "                        # Create the model, loss function, and optimizer\n",
        "                        model = LSTMNet(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
        "                                        output_size=1, dropout=dropout_size).to(device)\n",
        "                        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "                        criterion = nn.MSELoss()\n",
        "                        # Initialize the EarlyStopping callback\n",
        "                        early_stopping = pl.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
        "\n",
        "                        # Train the model\n",
        "                        trainer = pl.Trainer(max_epochs=num_epochs, gpus=1, callbacks=[early_stopping])\n",
        "                        trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "                        # Evaluate the model on the validation set for this fold\n",
        "                        val_loss = trainer.validate(val_loader)[0]['val_loss']\n",
        "                        val_losses.append(val_loss)\n",
        "\n",
        "                        mean_val_loss = np.mean(val_losses[:fold + 1])\n",
        "                        print(f\"Fold {fold + 1}/{num_folds}, Mean Validation Loss: {mean_val_loss:.6f}\")\n",
        "\n",
        "                        # Check if this model has a lower validation loss than the previous best model\n",
        "                        if mean_val_loss < best_val_loss:\n",
        "                            best_val_loss = mean_val_loss\n",
        "                            best_model = model\n",
        "                            best_hyperparams = (input_size, hidden_size, num_layers, dropout_size)\n",
        "\n",
        "    # Train the best model on the full training set and evaluate it on the test set\n",
        "    early_stopping = pl.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
        "    trainer = pl.Trainer(max_epochs=num_epochs, gpus=1, callbacks=[early_stopping])\n",
        "    trainer.fit(best_model, train_loader, val_loader)\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=8, shuffle=False)\n",
        "    test_loss = trainer.test(best_model, test_loader)[0]['test_loss']\n",
        "\n",
        "    print(f\"Best hyperparameters: input_size={best_hyperparams[0]}, hidden_size={best_hyperparams[1]}, num_layers={best_hyperparams[2]}, dropout={best_hyperparams[3]}\")\n",
        "    print(f\"Validation loss: {best_val_loss:.6f}\")\n",
        "    print(f\"Test loss: {test_loss:.6f}\")\n",
        "\n",
        "    # Save the best model with the best hyperparameters\n",
        "    torch.save(best_model.state_dict(), \"best_model.pt\")\n",
        "\n"
      ]
    }
  ]
}